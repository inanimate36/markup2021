train.dtm2 <- DocumentTermMatrix(corpus[index.train],
control = list(tokenize = UniBiTokenizer))
# 48392 uni+bigrams!
dim(train.dtm2) #[1]   640 48392
# remove terms that occur in less than 1% of documents
train.dtm2 <- removeSparseTerms(train.dtm2,0.99)
# after removing sparse terms only 317 left
dim(train.dtm2) #[1] 640 1597
reviews.glmnet2 <- cv.glmnet(as.matrix(train.dtm2),labels[index.train],
family="binomial",type.measure="class")
# show coefficient estimates for lambda-1se
# (only a selection of the bigram coefficients is shown here)
coef(reviews.glmnet2,s="lambda.1se")
# create document term matrix for the test data,
# using the training dictionary
test.dtm2 <- DocumentTermMatrix(corpus[-index.train],
control = list(tokenize=UniBiTokenizer,dictionary=Terms(train.dtm2)))
# make predictions using lambda.1se
reviews.glmnet.pred <- predict(reviews.glmnet2,newx=as.matrix(test.dtm2),
s="lambda.1se",type="class")
# accuracy improved due to including more unigrams and including bigrams!
table(reviews.glmnet.pred,labels[-index.train])
quality(labels[-index.train], reviews.glmnet.pred)
(63+72)/(63+72+8+17) # 84% this was with 0.99 for sparse terms
(57+70)/(57+70+23+10) # 79% #this is with .95 for sparse terms
train.mat <- as.matrix(train.dtm)
train.dat <- as.data.frame(train.mat)
train.dat$class <- labels[index.train]
reviews.tree <- rpart(class ~ ., data = train.dat, method = "class", cp = 0, minbucket = 1, minsplit = 1)
#summary(reviews.tree)
cp <- as.data.frame(reviews.tree$cptable)
cp[which.min(cp$xerror),]
reviews.tree <- rpart(class ~ ., data = train.dat, method = "class", cp = 0.009375)
test.mat <- as.matrix(test.dtm)
test.dat <- as.data.frame(test.mat)
dim(test.dat)
reviews.rpart.pred <- predict(object = reviews.tree, newdata = test.dat, type = "class")
quality(labels[-index.train], reviews.rpart.pred) #accuracy = 67.5
#data prep
train.mat2 <- as.matrix(train.dtm2)
train.dat2 <- as.data.frame(train.mat2)
train.dat2$class <- labels[index.train]
reviews.tree2 <- rpart(class ~ ., data = train.dat2, method = "class", cp = 0, minbucket = 1, minsplit = 1)
printcp(reviews.tree2)
plotcp(reviews.tree2)
#summary(reviews.tree2)
cp2 <- as.data.frame(reviews.tree2$cptable)
cp2[which.min(cp2$xerror),]
#fill in cp
reviews.tree2 <- rpart(class ~ ., data = train.dat2, method = "class", cp = 0.015625)
test.mat2 <- as.matrix(test.dtm2)
test.dat2 <- as.data.frame(test.mat2)
reviews.rpart.pred2 <- predict(object = reviews.tree2, newdata = test.dat2, type = "class")
quality(labels[-index.train], reviews.rpart.pred2) #accuracy = 65.625
quality(labels[-index.train], reviews.RF.pred) #81.875 accuracy
colnames(train.dat)[which(names(train.dat) == "else")] <- "ellse"
colnames(train.dat)[which(names(train.dat) == "next")] <- "nexxt"
colnames(train.dat)[which(names(train.dat) == "break")] <- "breakk"
#outcome has to be a factor for the function to perform a classification instead of regression tree
train.dat$class <- as.factor(train.dat$class)
colnames(test.dat)[which(names(test.dat) == "else")] <- "ellse"
colnames(test.dat)[which(names(test.dat) == "next")] <- "nexxt"
colnames(test.dat)[which(names(test.dat) == "break")] <- "breakk"
plot(forest5$results$mtry, forest5$results$Accuracy)
forest5$bestTune
?randomForest
plot(forest4$results$min.node.size, forest4$results$Accuracy)
forest <- randomForest(class ~ ., data = train.dat, mtry = 6, nodesize = 26)
quality(labels[-index.train], reviews.RF.pred)
reviews.RF.pred <- predict(object = forest, newdata = test.dat, type = "class")
quality(labels[-index.train], reviews.RF.pred)
?randomForest
forest <- randomForest(class ~ ., data = train.dat, mtry = 6, nodesize = 26, ntree = 1000)
reviews.RF.pred <- predict(object = forest, newdata = test.dat, type = "class")
quality(labels[-index.train], reviews.RF.pred)
tuneGrid <- expand.grid(mtry = c(seq(1,100)),
min.node.size = 1,
splitrule = "gini")
forest5 <- train(class ~ ., data = train.dat2, method = "ranger", metric= "Accuracy", trControl = trControl, tuneGrid = tuneGrid)
###### Bigrams for RF #########
train.dat2$class <- as.factor(train.dat2$class)
forest5 <- train(class ~ ., data = train.dat2, method = "ranger", metric= "Accuracy", trControl = trControl, tuneGrid = tuneGrid)
forest5$bestTune
tuneGrid <- expand.grid(mtry = 10,
min.node.size = c(seq(1,50)),
splitrule = "gini")
forest7 <- train(class ~ ., data = train.dat2, method = "ranger", metric= "Accuracy", trControl = trControl, tuneGrid = tuneGrid)
forest7$bestTune
plot(forest7$results$mtry, forest7$results$Accuracy)
plot(forest7$results$min.node.size, forest7$results$Accuracy)
forest7$bestTune
reviews.RF.pred <- predict(object = forest, newdata = test.dat2, type = "class")
### try with ranger
model <- ranger(class ~ ., data = train.dat, mtry = 6, min.node.size = 26)
model
?ranger
forest6 <- train(class ~ ., data = train.dat2, method = "ranger", metric= "Accuracy", trControl = trControl, tuneGrid = tuneGrid, importance = "impurity")
plot(forest6$results$mtry, forest6$results$Accuracy)
forest6
plot(forest6$results$min.node.size, forest6$results$Accuracy)
forest4 <- train(class ~ ., data = train.dat, method = "ranger", metric= "Accuracy", trControl = trControl, tuneGrid = tuneGrid,  importance = "impurity")
plot(forest4$results$min.node.size, forest4$results$Accuracy)
forest4$bestTune
forest5 <- train(class ~ ., data = train.dat, method = "ranger", metric= "Accuracy", trControl = trControl, tuneGrid = tuneGrid,  importance = "impurity")
plot(forest5$results$mtry, forest5$results$Accuracy)
tuneGrid <- expand.grid(mtry = c(seq(1,100)),
min.node.size = 26,
splitrule = "gini")
forest5 <- train(class ~ ., data = train.dat, method = "ranger", metric= "Accuracy", trControl = trControl, tuneGrid = tuneGrid,  importance = "impurity")
plot(forest5$results$mtry, forest5$results$Accuracy)
forest5$bestTune
# try with ranger
model <- ranger(class ~ ., data = train.dat, mtry = 24, min.node.size = 26)
reviews.RF.pred <- predict(object = forest, newdata = test.dat, type = "class")
quality(labels[-index.train], reviews.RF.pred)
# try with ranger
model <- ranger(class ~ ., data = train.dat, mtry = 24, min.node.size = 26, importance = "impurity")
reviews.RF.pred <- predict(object = forest, newdata = test.dat, type = "class")
reviews.ranger.pred <- predict(object = model, newdata = test.dat, type = "class")
reviews.ranger.pred <- predict(object = model, data = test.dat, type = "class")
reviews.ranger.pred <- predict(object = model, data = test.dat)
reviews.ranger.pred
reviews.ranger.pred$predictions
quality(labels[-index.train], reviews.ranger.pred$predictions)
colnames(train.dat)[which(names(train.dat) == "else")] <- "ellse"
colnames(train.dat)[which(names(train.dat) == "next")] <- "nexxt"
colnames(train.dat)[which(names(train.dat) == "break")] <- "breakk"
# change colnames in test.dat so that they can be used for prediction function
colnames(test.dat)[which(names(test.dat) == "else")] <- "ellse"
colnames(test.dat)[which(names(test.dat) == "next")] <- "nexxt"
colnames(test.dat)[which(names(test.dat) == "break")] <- "breakk"
forest7 <- train(class ~ ., data = train.dat2, method = "ranger", metric= "Accuracy", trControl = trControl, tuneGrid = tuneGrid, importance = "impurity")
forest7$bestTune
plot(forest7$results$min.node.size, forest7$results$Accuracy)
tuneGrid <- expand.grid(mtry = c(seq(1,100)),
min.node.size = 1,
splitrule = "gini")
forest6 <- train(class ~ ., data = train.dat2, method = "ranger", metric= "Accuracy", trControl = trControl, tuneGrid = tuneGrid, importance = "impurity")
plot(forest7$results$min.node.size, forest7$results$Accuracy)
plot(forest6$results$min.node.size, forest6$results$Accuracy)
plot(forest6$results$mtry, forest6$results$Accuracy)
forest6$bestTune
tuneGrid <- expand.grid(mtry = 8,
min.node.size = c(seq(1,50)),
splitrule = "gini")
forest7 <- train(class ~ ., data = train.dat2, method = "ranger", metric= "Accuracy", trControl = trControl, tuneGrid = tuneGrid, importance = "impurity")
plot(forest7$results$min.node.size, forest7$results$Accuracy)
forest7$bestTune
forest2 <- randomForest(class ~ ., data = train.dat2, mtry = 8, nodesize = 4, ntree = 1000)
cond.probs$`1`
cond.probs$score <- log(cond.probs$`1`) - log(cond.probs$`0`)
cond.probs$score
cond.probs
reviews.RF.pred <- predict(object = forest, newdata = test.dat2, type = "class")
forest2 <- randomForest(class ~ ., data = train.dat2, mtry = 8, nodesize = 4, ntree = 1000)
/// optimal tunes
colnames(train.dat2)[which(names(train.dat2) == "else")] <- "ellse"
colnames(train.dat2)[which(names(train.dat2) == "next")] <- "nexxt"
colnames(train.dat2)[which(names(train.dat2) == "break")] <- "breakk"
# change colnames in test.dat so that they can be used for prediction function
colnames(test.dat2)[which(names(test.dat2) == "else")] <- "ellse"
colnames(test.dat2)[which(names(test.dat2) == "next")] <- "nexxt"
colnames(test.dat2)[which(names(test.dat2) == "break")] <- "breakk"
forest2 <- randomForest(class ~ ., data = train.dat2, mtry = 8, nodesize = 4, ntree = 1000)
colnames(test.dat2)[which(names(test.dat2) == "able get")] <- "able gett"
forest2 <- randomForest(class ~ ., data = train.dat2, mtry = 8, nodesize = 4, ntree = 1000)
colnames(train.dat2)[which(names(train.dat2) == "able get")] <- "able gett"
forest2 <- randomForest(class ~ ., data = train.dat2, mtry = 8, nodesize = 4, ntree = 1000)
colnames(train.dat2)[which(names(train.dat2) == "able get")] <- "ablle gett"
colnames(test.dat2)[which(names(test.dat2) == "able get")] <- "ablle gett"
forest2 <- randomForest(class ~ ., data = train.dat2, mtry = 8, nodesize = 4, ntree = 1000)
train.dat2$abl
forest2 <- randomForest(class ~ ., data = train.dat2, mtry = 8, nodesize = 4, ntree = 1000)
colnames(train.dat2)[which(names(train.dat2) == "able gett")] <- "ablle gett"
colnames(test.dat2)[which(names(test.dat2) == "able gett")] <- "ablle gett"
forest2 <- randomForest(class ~ ., data = train.dat2, mtry = 8, nodesize = 4, ntree = 1000)
sum(train.dat2$`ablle gett`)
class(train.dat2$class)
forest2 <- randomForest(class ~ ., data = train.dat2, mtry = 8, nodesize = 4, ntree = 1000)
class(train.dat2)
model2 <- ranger(class ~ ., data = train.dat2, mtry = 8, min.node.size = 4, importance = "impurity")
try <- test.dat2
View(try)
names(try) <- gsub(" ", "_", names(try))
View(try)
forest2 <- randomForest(class ~ ., data = try, mtry = 8, nodesize = 4, ntree = 1000)
class(try$class)
try$class <- as.factor(try$class)
forest2 <- randomForest(class ~ ., data = try, mtry = 8, nodesize = 4, ntree = 1000, )
forest2
reviews.RF.pred <- predict(object = forest2, newdata = test.dat2, type = "class")
names(train.dat2) <- gsub(" ", "_", names(try))
names(test.dat2) <- gsub(" ", "_", names(try))
train.dat2$class <- as.factor(train.dat2$class)
test.dat2$class <- as.factor(test.dat2$class)
forest2 <- randomForest(class ~ ., data = train.dat2, mtry = 8, nodesize = 4, ntree = 1000 )
reviews.RF.pred <- predict(object = forest2, newdata = test.dat2, type = "class")
reviews.RF.pred2 <- predict(object = forest2, newdata = test.dat2, type = "class")
quality(labels[-index.train], reviews.RF.pred2)
nodesize4 <- quality(labels[-index.train], reviews.RF.pred2)
forest2 <- randomForest(class ~ ., data = train.dat2, mtry = 8, ntree = 1000)
reviews.RF.pred2 <- predict(object = forest2, newdata = test.dat2, type = "class")
nodesize1 <- quality(labels[-index.train], reviews.RF.pred2)
nodesize1 == nodesize4
nodesize1
forest2 <- randomForest(class ~ ., data = train.dat2, ntree = 1000)
reviews.RF.pred2 <- predict(object = forest2, newdata = test.dat2, type = "class")
mtry1 <- quality(labels[-index.train], reviews.RF.pred2)
ncol(train.dat2)
mtrydefault <- quality(labels[-index.train], reviews.RF.pred2)
mtrydefault
nodesize1
nodesize4
mtrydefault
View(reviews.glmnet)
nodesize4
forest <- randomForest(class ~ ., data = train.dat, mtry = 6, nodesize = 26, ntree = 1000)
reviews.RF.pred <- predict(object = forest, newdata = test.dat, type = "class")
#table(reviews.RF.pred, labels[-index.train])
mtry6ns26 <- quality(labels[-index.train], reviews.RF.pred)
forest <- randomForest(class ~ ., data = train.dat, ntree = 1000)
reviews.RF.pred <- predict(object = forest, newdata = test.dat, type = "class")
#table(reviews.RF.pred, labels[-index.train])
forestdefault <- quality(labels[-index.train], reviews.RF.pred)
mtry6ns26
forestdefault
forest <- randomForest(class ~ ., data = train.dat, mtry = 6, ntree = 1000)
reviews.RF.pred <- predict(object = forest, newdata = test.dat, type = "class")
#table(reviews.RF.pred, labels[-index.train])
mtry6 <- quality(labels[-index.train], reviews.RF.pred)
mtry6
mtry6
plot(forest5$results$mtry, forest5$results$Accuracy)
plot(forest4$results$min.node.size, forest4$results$Accuracy)
nodesize4
nodesize1
mtrydefault
nodesize1
forest5$bestTune
forest4$bestTune
dim(train.dat2)
dim(train.dat)
reviews.RF.pred2
# first, we create a dataframe that has a column for each algorithm with the row-values 0 and 1
# for incorrect vs. correct predictions
accurate_predictions <- data.frame(case = seq(1, length(reviews.RF.pred2)), preds_RF1_correct = NA,
preds_RF2_correct = NA, preds_rpart_correct = NA, preds_rpart2_correct = NA) %>%
mutate(preds_RF1_correct = ifelse(reviews.RF.pred == labels[-index.train], 1, 0),
preds_RF2_correct = ifelse(reviews.RF.pred2 == labels[-index.train], 1, 0),
preds_rpart_correct = ifelse(reviews.rpart.pred == labels[-index.train], 1, 0),
preds_rpart2_correct = ifelse(reviews.rpart.pred2 == labels[-index.train], 1, 0))
library(tidyr)
# first, we create a dataframe that has a column for each algorithm with the row-values 0 and 1
# for incorrect vs. correct predictions
accurate_predictions <- data.frame(case = seq(1, length(reviews.RF.pred2)), preds_RF1_correct = NA,
preds_RF2_correct = NA, preds_rpart_correct = NA, preds_rpart2_correct = NA) %>%
mutate(preds_RF1_correct = ifelse(reviews.RF.pred == labels[-index.train], 1, 0),
preds_RF2_correct = ifelse(reviews.RF.pred2 == labels[-index.train], 1, 0),
preds_rpart_correct = ifelse(reviews.rpart.pred == labels[-index.train], 1, 0),
preds_rpart2_correct = ifelse(reviews.rpart.pred2 == labels[-index.train], 1, 0))
library(tidyr)
# first, we create a dataframe that has a column for each algorithm with the row-values 0 and 1
# for incorrect vs. correct predictions
accurate_predictions <- data.frame(case = seq(1, length(reviews.RF.pred2)), preds_RF1_correct = NA,
preds_RF2_correct = NA, preds_rpart_correct = NA, preds_rpart2_correct = NA) %>%
mutate(preds_RF1_correct = ifelse(reviews.RF.pred == labels[-index.train], 1, 0),
preds_RF2_correct = ifelse(reviews.RF.pred2 == labels[-index.train], 1, 0),
preds_rpart_correct = ifelse(reviews.rpart.pred == labels[-index.train], 1, 0),
preds_rpart2_correct = ifelse(reviews.rpart.pred2 == labels[-index.train], 1, 0))
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
# first, we create a dataframe that has a column for each algorithm with the row-values 0 and 1
# for incorrect vs. correct predictions
accurate_predictions <- data.frame(case = seq(1, length(reviews.RF.pred2)), preds_RF1_correct = NA,
preds_RF2_correct = NA, preds_rpart_correct = NA, preds_rpart2_correct = NA) %>%
mutate(preds_RF1_correct = ifelse(reviews.RF.pred == labels[-index.train], 1, 0),
preds_RF2_correct = ifelse(reviews.RF.pred2 == labels[-index.train], 1, 0),
preds_rpart_correct = ifelse(reviews.rpart.pred == labels[-index.train], 1, 0),
preds_rpart2_correct = ifelse(reviews.rpart.pred2 == labels[-index.train], 1, 0))
library(stats)
# next, we perform pairwise  mcnemar tests
mcnemar.test(accurate_predictions$preds_RF1_correct, accurate_predictions$preds_RF2_correct)
mcnemar.test(accurate_predictions$reviews.rpart.pred, accurate_predictions$reviews.rpart.pred2)
reviews.rpart.pred2
mcnemar.test(accurate_predictions$preds_rpart_correct, accurate_predictions$preds_rpart2_correct)
# next, we perform pairwise  mcnemar tests
mcnemar.test(accurate_predictions$preds_RF1_correct, accurate_predictions$preds_RF2_correct, correct = F)
mcnemar.test(accurate_predictions$preds_rpart_correct, accurate_predictions$preds_rpart2_correct, correct = F)
paste(reviews.RF.pred, collapse = ",")
accurate_predictions$preds_RF1_correct
paste(accurate_predictions$preds_RF1_correct, collapse = ",")
forest <- randomForest(class ~ ., data = train.dat, mtry = 6, nodesize = 26, ntree = 1000)
reviews.RF.pred <- predict(object = forest, newdata = test.dat, type = "class")
# first, we create a dataframe that has a column for each algorithm with the row-values 0 and 1
# for incorrect vs. correct predictions
accurate_predictions <- data.frame(case = seq(1, length(reviews.RF.pred2)), preds_RF1_correct = NA,
preds_RF2_correct = NA, preds_rpart_correct = NA, preds_rpart2_correct = NA) %>%
mutate(preds_RF1_correct = ifelse(reviews.RF.pred == labels[-index.train], 1, 0),
preds_RF2_correct = ifelse(reviews.RF.pred2 == labels[-index.train], 1, 0),
preds_rpart_correct = ifelse(reviews.rpart.pred == labels[-index.train], 1, 0),
preds_rpart2_correct = ifelse(reviews.rpart.pred2 == labels[-index.train], 1, 0))
# next, we perform pairwise  mcnemar tests
mcnemar.test(accurate_predictions$preds_RF1_correct, accurate_predictions$preds_RF2_correct, correct = F)
paste(accurate_predictions$preds_RF1_correct, collapse = ",")
sum(accurate_predictions$preds_RF1_correct) / length(accurate_predictions$preds_RF1_correct)
forest <- randomForest(class ~ ., data = train.dat, mtry = 2)
tuneGrid <- expand.grid(mtry = 6,
min.node.size = c(seq(1, 100)),
splitrule = "gini")
forest4 <- train(class ~ ., data = train.dat, method = "ranger", metric= "Accuracy", trControl = trControl, tuneGrid = tuneGrid,  importance = "impurity")
forest4$bestTune
tuneGrid <- expand.grid(mtry = c(seq(1,100)),
min.node.size = 26,
splitrule = "gini")
forest5 <- train(class ~ ., data = train.dat, method = "ranger", metric= "Accuracy", trControl = trControl, tuneGrid = tuneGrid,  importance = "impurity")
forest5$bestTune
plot(forest5$results$mtry, forest5$results$Accuracy)
forest5$bestTune
forest <- randomForest(class ~ ., data = train.dat, mtry = 15, nodesize = 26, ntree = 1000)
reviews.RF.pred <- predict(object = forest, newdata = test.dat, type = "class")
quality(labels[-index.train], reviews.RF.pred)
mtry6ns26
mtry6
forestdefault
forest <- randomForest(class ~ ., data = train.dat, mtry = 6, ntree = 1000)
tuneGrid <- expand.grid(mtry = c(seq(1,100)),
min.node.size = 1,
splitrule = "gini")
forest5 <- train(class ~ ., data = train.dat, method = "ranger", metric= "Accuracy", trControl = trControl, tuneGrid = tuneGrid,  importance = "impurity")
plot(forest5$results$mtry, forest5$results$Accuracy)
forest5$bestTune
forestagain <- randomForest(class ~ ., data = train.dat, mtry = 15, ntree = 1000)
reviews.RF.pred <- predict(object = forestagain, newdata = test.dat, type = "class")
quality(labels[-index.train], reviews.RF.pred)
forestagain <- randomForest(class ~ ., data = train.dat, mtry = 20, ntree = 1000)
forestagain <- randomForest(class ~ ., data = train.dat, mtry = 10, ntree = 1000)
reviews.RF.pred <- predict(object = forestagain, newdata = test.dat, type = "class")
quality(labels[-index.train], reviews.RF.pred)
# first, we create a dataframe that has a column for each algorithm with the row-values 0 and 1
# for incorrect vs. correct predictions
accurate_predictions <- data.frame(case = seq(1, length(reviews.RF.pred2)), preds_RF1_correct = NA,
preds_RF2_correct = NA, preds_rpart_correct = NA, preds_rpart2_correct = NA) %>%
mutate(preds_RF1_correct = ifelse(reviews.RF.pred == labels[-index.train], 1, 0),
preds_RF2_correct = ifelse(reviews.RF.pred2 == labels[-index.train], 1, 0),
preds_rpart_correct = ifelse(reviews.rpart.pred == labels[-index.train], 1, 0),
preds_rpart2_correct = ifelse(reviews.rpart.pred2 == labels[-index.train], 1, 0))
quality(reviews.RF.pred, labels[-index.train])
forestagainagain <- randomForest(class ~ ., data = train.dat, mtry = 10, ntree = 1000)
reviews.RF.predagain <- predict(object = forestagain, newdata = test.dat, type = "class")
quality(reviews.RF.predagain, labels[-index.train])
reviews.RF.predagain <- predict(object = forestagainagain, newdata = test.dat, type = "class")
quality(reviews.RF.predagain, labels[-index.train])
set.seed(100)
forestagainagain <- randomForest(class ~ ., data = train.dat, mtry = 10, ntree = 1000)
reviews.RF.predagain <- predict(object = forestagainagain, newdata = test.dat, type = "class")
quality(reviews.RF.predagain, labels[-index.train])
somethingsfishy <- ifelse(reviews.RF.predagain == labels[-index.train], 1, 0)
somethingsfishy
paste(somethingsfishy, collapse = ",")
somethingsfishy <- ifelse(reviews.RF.pred == labels[-index.train], 1, 0)
paste(somethingsfishy, collapse = ",")
# first, we create a dataframe that has a column for each algorithm with the row-values 0 and 1
# for incorrect vs. correct predictions
accurate_predictions <- data.frame(case = seq(1, length(reviews.RF.pred2)), preds_RF1_correct = NA,
preds_RF2_correct = NA, preds_rpart_correct = NA, preds_rpart2_correct = NA) %>%
mutate(preds_RF1_correct = ifelse(reviews.RF.predagain == labels[-index.train], 1, 0),
preds_RF2_correct = ifelse(reviews.RF.pred2 == labels[-index.train], 1, 0),
preds_rpart_correct = ifelse(reviews.rpart.pred == labels[-index.train], 1, 0),
preds_rpart2_correct = ifelse(reviews.rpart.pred2 == labels[-index.train], 1, 0))
# next, we perform pairwise  mcnemar tests
mcnemar.test(accurate_predictions$preds_RF1_correct, accurate_predictions$preds_RF2_correct, correct = F)
rpart
cite(rpart)
cite(r)
which(rpart)
citation(rpart)
citation(package = "rpart")
citation(package = "randomForest")
citation(package 0 "caret")
citation(package = "caret")
sum(accurate_predictions$preds_rpart2_correct)/length(accurate_predictions$preds_rpart2_correct)
accurate_predictions$preds_rpart2_correct
sum(accurate_predictions$preds_RF2_correct)/length(accurate_predictions$preds_RF2_correct)
forest2
forest2 <- randomForest(class ~ ., data = train.dat2, mtry = 8, ntree = 1000) #mtry was 8
reviews.RF.pred2 <- predict(object = forest2, newdata = test.dat2, type = "class")
quality(labels[-index.train], reviews.RF.pred2)
nodesize4
nodesize1
forest2 <- randomForest(class ~ ., data = train.dat2, mtry = 8, ntree = 1000) #mtry was 8
reviews.RF.pred2 <- predict(object = forest2, newdata = test.dat2, type = "class")
quality(labels[-index.train], reviews.RF.pred2)
# first, we create a dataframe that has a column for each algorithm with the row-values 0 and 1
# for incorrect vs. correct predictions
accurate_predictions <- data.frame(case = seq(1, length(reviews.RF.pred2)), preds_RF1_correct = NA,
preds_RF2_correct = NA, preds_rpart_correct = NA, preds_rpart2_correct = NA) %>%
mutate(preds_RF1_correct = ifelse(reviews.RF.predagain == labels[-index.train], 1, 0),
preds_RF2_correct = ifelse(reviews.RF.pred2 == labels[-index.train], 1, 0),
preds_rpart_correct = ifelse(reviews.rpart.pred == labels[-index.train], 1, 0),
preds_rpart2_correct = ifelse(reviews.rpart.pred2 == labels[-index.train], 1, 0))
# next, we perform pairwise  mcnemar tests
mcnemar.test(accurate_predictions$preds_RF1_correct, accurate_predictions$preds_RF2_correct, correct = F)
citation(package = "tm")
forestagainagain <- randomForest(class ~ ., data = train.dat, mtry = 10, ntree = 2000)
reviews.RF.predagain <- predict(object = forestagainagain, newdata = test.dat, type = "class")
quality(reviews.RF.predagain, labels[-index.train])
x = (1,2,3,4,5,6,7,8,9)
x = c(1,2,3,4,5,6,7,8,9)
x
print(x[i])
for i in x{
print(x[i])
}
for(i in x){
print(x[i])
}
print(x[i])
for(i in 1:9){
print(x[i])
}
y = c(a, b, c, d, e, f, g, h,i)
y = c("a", "b", "c", "d", "e", "f", "g", "h","i")
z = cbind(x, y)
z
for(i in x){
print(x[i, x])
}
for(i in x){
print(x[i, 1])
}
for(i in x){
print(x[i, 1])
}
for(i in x){
print(z[i, 1])
}
for(i in z){
print(z[i, 1])
}
for(i in z){
print(z[i, 1])
}
z
for(i in z){
print(z[i, 1])
}
for(i in z$x){
print(z[i, 1])
}
for(i in z[,1]){
print(z[i, 1])
}
1086922+184394+1795364+5705398+1404603+2939831+5416572+8415116+2143648+5517976+790293+4883503
198/7775
384/7775
355/7775
1074/7775
259/7775
526/7775
1110/7775
1462/7775
421/7775
971/7775
133/7775
882/7775
1086922/41942720
184394/41942720
1795364/41942720
5705398/41942720
1404603/41942720
2939831/41942720
5416572/41942720
8415116/41942720
2143648/41942720
5517976/41942720
790293/41942720
4883503/41942720
sample(x=c('Domplein', 'Ganzenmarkt', 'Mariaplaats', 'Neude', 'Vredenburg'), size=1, prob=rep(.2, times=5))
getwd()
setwd("/Users/inanb/Documents/GitHub/markup2021/Inan /Shinyapp_exercise")
library(shiny)
runApp("my_app")
runApp("app.R")
runApp(app.R)
runApp("App-1")
runApp("App-1")
getcd()
getwd()
runApp('App-1/app')
runApp('App-1/app')
runApp('App-1/app')
data(mtcars)
View(mtcars)
mtcars
runApp('App-1/app')
runApp('App-1/app')
runApp('App-1/app')
runApp('App-1/app')
runApp('App-1/app')
install.packages('rsconnect')
install.packages('rsconnect')
rsconnect::setAccountInfo(name='inanimate36', token='D5216964ACCC00F247766C071238C30C', secret='TkYnKFIwCkNya21JSCkIH8JJh4z/+FUvEbrwHPok')
runApp('App-1/app')
rsconnect::setAccountInfo(name='inanimate36', token='D5216964ACCC00F247766C071238C30C', secret='TkYnKFIwCkNya21JSCkIH8JJh4z/+FUvEbrwHPok')
runApp('App-1/app')
rsconnect::deployApp()
getwd()
setwd("/Users/inanb/Documents/GitHub/markup2021/Inan /Shinyapp_exercise/App-1/app")
rsconnect::deployApp()
setwd("/Users/inanb/Documents/GitHub/markup2021/Inan /Shinyapp_exercise/App-1/app")
rsconnect::deployApp()
